
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction to Stochastic Processes &#8212; FRE 6083: Quantitative Methods</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\C": "\\mathbb{C}", "\\E": "\\mathbb{E}", "\\P": "\\mathbb{P}", "\\var": "\\text{var}", "\\cov": "\\text{cov}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/lecture06_stochastic-processes';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Continuous-Time Limit of the Random Walk" href="lecture07_brownian-motion.html" />
    <link rel="prev" title="The Binomial Asset Pricing Model" href="lecture05_binomial-asset-pricing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">FRE 6083: Quantitative Methods</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture01_sequences-and-sums.html">Lecture 1: Sequences and Sums of Random Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture02_convergence-and-limits.html">Lecture 2: Convergence and Limit Theorems</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture03_markov-chains.html">Lecture 3: Markov Chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture04_poisson-process.html">Lecture 4: The Poisson Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture05_binomial-asset-pricing.html">Lecture 5: Binomial Asset Pricing Model</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 6: Stochastic Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture07_brownian-motion.html">Lecture 7: Random Walk Limit</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture09_brownian-motion.html">Lecture 9: Brownian Motion</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture10_stochastic-calculus.html">Lecture 10: Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture11_black-scholes-model.html">Lecture 11: Black-Scholes Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture12_black-scholes-pde-finite-difference.html">Lecture 12: Black-Scholes PDE and Finite Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture13_portfolio-optimization-discrete-time.html">Lecture 13: Portfolio Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture14_american-options-greeks.html">Lecture 14: American Options and Greeks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/duymlcoding/NYU_FRE_Notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/duymlcoding/NYU_FRE_Notes/edit/main/chapters/lecture06_stochastic-processes.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/duymlcoding/NYU_FRE_Notes/issues/new?title=Issue%20on%20page%20%2Fchapters/lecture06_stochastic-processes.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/lecture06_stochastic-processes.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Stochastic Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-concepts-and-examples">Preliminary Concepts and Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-framework">The Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-stochastic-process">Definition of Stochastic Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space">State Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-elementary-examples">Some Elementary Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-symmetric-random-walk">Example of the Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-step-increment">One-Step Increment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk-definition">Random Walk Definition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-and-second-moments">First and Second Moments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-of-a-stochastic-process">Distribution of a Stochastic Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-function-of-order-k">Distribution Function of Order <span class="math notranslate nohighlight">\(k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-and-continuous-cases">Discrete and Continuous Cases</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-stationarity-of-increments">Independence and Stationarity of Increments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-the-bernoulli-process-revisited">An Example: The Bernoulli Process Revisited</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-and-expectation">Distribution and Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-successes">Cumulative Successes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-stationarity">Independence and Stationarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#another-important-example-the-symmetric-random-walk">Another Important Example: The Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moments-of-increments">Moments of Increments</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-mean-and-variance">Overall Mean and Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autocovariance-and-autocorrelation-functions">Autocovariance and Autocorrelation Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-ar-1-process-for-a-1">Example of the AR(1) Process for <span class="math notranslate nohighlight">\(a = 1\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-autocovariance-function">Computing the Autocovariance Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strict-sense-and-wide-sense-stationarity">Strict Sense and Wide Sense Stationarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-between-sss-and-wss">Relationship Between SSS and WSS</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-stationarity">Examples of Stationarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-4-the-symmetric-random-walk">Example 4: The Symmetric Random Walk</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ar-1-process">The AR(1) Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explicit-form">Explicit Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-processes">The Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-normal-distribution">Multi-Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution">Joint Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#density-function">Density Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-covariance-matrix">Properties of Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes-and-stationarity">Gaussian Processes and Stationarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ergodicity">Ergodicity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-mean">Temporal Mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-ergodic-definition">Mean-Ergodic Definition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-ergodicity">Examples of Ergodicity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-features-of-real-financial-data">The Features of Real Financial Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-returns">Log Returns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stylized-facts">Stylized Facts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-index">Tail Index</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-empirical-observations">Other Empirical Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-assumptions-and-their-limitations">Statistical Assumptions and Their Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationarity-in-practice">Stationarity in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ergodicity-in-practice">Ergodicity in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-sample-considerations">Finite Sample Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-problems">Practice Problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1">Problem 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2">Problem 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3">Problem 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4">Problem 4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-5">Problem 5</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-stochastic-processes">
<h1>Introduction to Stochastic Processes<a class="headerlink" href="#introduction-to-stochastic-processes" title="Link to this heading">#</a></h1>
<p>Stochastic processes are used for modeling the time evolution of financial assets. In this lecture, we present some of the basic mathematical concepts and definitions of the theory of stochastic processes, such as stationarity and independence of increments. These concepts constitute the foundation for conducting empirical studies and building sound mathematical models.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><strong>Recommended References:</strong></p>
<ul class="simple">
<li><p>Jean Jacod and Philip Protter, “Probability Essentials”, Universitext, Second Edition, 2004, Springer</p></li>
<li><p>Paul Wilmott, Sam Howison, and Jeff Dewynne, “Mathematics of Financial Derivatives: A Student Introduction”, Cambridge University Press, 1995</p></li>
<li><p>Rama Cont, “Empirical properties of asset returns: stylized facts and statistical issues”, Quantitative Finance, 1:223-236, 2001</p></li>
<li><p>Dan Stefanica, “A Linear Algebra Primer for Financial Engineering: Covariance Matrices, Eigenvectors, OLS, and more”, Financial Engineering Advanced Background Series, 2014, FE Press New York</p></li>
</ul>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In this course, we have already encountered some well-known examples of stochastic processes, i.e., random variables that depend on time.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Examples We’ve Seen:</strong></p>
<ul class="simple">
<li><p><strong>Sequences of random variables:</strong> Stochastic processes in discrete time</p></li>
<li><p><strong>Markov chains:</strong> Stochastic processes with a discrete state space that satisfy the Markov property</p></li>
<li><p><strong>Random walks:</strong> Fundamental discrete-time processes</p></li>
<li><p><strong>Poisson process:</strong> A continuous-time counting process</p></li>
</ul>
</div>
<p>We illustrate these concepts with an array of elementary examples, as well as examples that constitute the building blocks of many financial models: the random walk, the Poisson process, and the autoregressive model in Econometrics.</p>
<p>We also discuss the features of real financial data that have been discovered through empirical studies. Unfortunately, it is very difficult to come up with a model that possesses the same features as real financial data.</p>
<p>In addition to these notes, we refer for this discussion to the very well written and pedagogical article by Rama Cont [3].</p>
</section>
<section id="preliminary-concepts-and-examples">
<h2>Preliminary Concepts and Examples<a class="headerlink" href="#preliminary-concepts-and-examples" title="Link to this heading">#</a></h2>
<section id="the-framework">
<h3>The Framework<a class="headerlink" href="#the-framework" title="Link to this heading">#</a></h3>
<p>We consider a random experiment <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>, i.e., an experiment that can be repeated under the same conditions, and whose result cannot be predicted with certainty.</p>
<p>Next, we denote as before, by <span class="math notranslate nohighlight">\(\Omega\)</span>, the <strong>sample space</strong>, i.e., the set of possible outcomes for the experiment <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>.</p>
<p>We model the time by using a set <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>. Generally, we will either have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{T} = [0, +\infty)\)</span> for a continuous-time process</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{T} = \{0, 1, 2, 3, \ldots\}\)</span> for a discrete-time process</p></li>
</ul>
</section>
<section id="definition-of-stochastic-process">
<h3>Definition of Stochastic Process<a class="headerlink" href="#definition-of-stochastic-process" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Stochastic Process</strong></p>
<p>A stochastic process is defined as a function <span class="math notranslate nohighlight">\(X\)</span> of the time and of an outcome, <span class="math notranslate nohighlight">\(X(t, \omega)\)</span> where <span class="math notranslate nohighlight">\(t \in \mathcal{T}\)</span>, <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>.</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(\omega\)</span> <strong>fixed</strong>, <span class="math notranslate nohighlight">\(\{X(t, \omega); t \in \mathcal{T}\}\)</span> is called a <strong>sample path</strong></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> <strong>fixed</strong>, <span class="math notranslate nohighlight">\(X(t)\)</span> is a <strong>random variable</strong></p></li>
</ul>
</div>
<p><strong>Physical Meaning:</strong> A stochastic process is a family of random variables indexed by time. You can think of it either as:</p>
<ul class="simple">
<li><p>A collection of random variables (one for each time <span class="math notranslate nohighlight">\(t\)</span>), or</p></li>
<li><p>A random function of time (one function for each outcome <span class="math notranslate nohighlight">\(\omega\)</span>)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Alternative View:</strong></p>
<p>Roughly, a stochastic process is just a sequence of random variables. However, in continuous-time, we are dealing with a <strong>continuum</strong> of random variables indexed by the continuous time variable <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div>
</section>
<section id="state-space">
<h3>State Space<a class="headerlink" href="#state-space" title="Link to this heading">#</a></h3>
<p>Often, the notation <span class="math notranslate nohighlight">\(S_X\)</span> is used for the <strong>state space</strong>, which is the set of values that the stochastic process <span class="math notranslate nohighlight">\(X\)</span> can take.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Classification by State Space:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(S_X\)</span> is finite or countably infinite, <span class="math notranslate nohighlight">\(X\)</span> is said to be a <strong>discrete-state process</strong></p>
<ul>
<li><p>Examples: <span class="math notranslate nohighlight">\(S_X = \{0, 1, 2, \ldots, N\}\)</span>, <span class="math notranslate nohighlight">\(S_X = \mathbb{N}\)</span>, or <span class="math notranslate nohighlight">\(S_X = \mathbb{Z}\)</span></p></li>
</ul>
</li>
<li><p>If <span class="math notranslate nohighlight">\(S_X\)</span> is uncountably infinite (for instance <span class="math notranslate nohighlight">\(S_X = \mathbb{R}\)</span>), <span class="math notranslate nohighlight">\(X\)</span> is said to be a <strong>continuous-state process</strong></p></li>
</ul>
</div>
</section>
</section>
<section id="some-elementary-examples">
<h2>Some Elementary Examples<a class="headerlink" href="#some-elementary-examples" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example 1: Linear Growth</p>
<p>Consider the continuous-time process <span class="math notranslate nohighlight">\(X(t, \omega) = tY(\omega)\)</span> where <span class="math notranslate nohighlight">\(t \in (0, +\infty)\)</span> and <span class="math notranslate nohighlight">\(Y &gt; 0\)</span> is a random variable.</p>
<p>Since <span class="math notranslate nohighlight">\(S_X = [0, +\infty)\)</span>, it is a <strong>continuous-state process</strong>.</p>
<p><strong>Physical Meaning:</strong> The process grows linearly in time with a random growth rate <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 2: Time-Independent Process</p>
<p>Consider the trivial stochastic process <span class="math notranslate nohighlight">\(X(t, \omega) = Y(\omega)\)</span>, where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable.</p>
<p>Here, the stochastic process is actually entirely time-independent and boils down to a random variable.</p>
<p><strong>Physical Meaning:</strong> The value is random but doesn’t change over time once realized.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 3: Deterministic Process</p>
<p>Consider the trivial stochastic process <span class="math notranslate nohighlight">\(X(t, \omega) = c\)</span>, where <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span> is just a constant.</p>
<p><strong>Physical Meaning:</strong> No randomness, just a constant value for all times and all outcomes.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 4: Deterministic Function of Time</p>
<p>Consider another trivial stochastic process <span class="math notranslate nohighlight">\(X(t, \omega) = t^2\)</span>.</p>
<p>Here <span class="math notranslate nohighlight">\(X\)</span> is just a deterministic function of the real variable <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><strong>Physical Meaning:</strong> The process evolves deterministically over time with no randomness.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 5: Exponential Decay</p>
<p>Consider the process <span class="math notranslate nohighlight">\(X(t, \omega) = e^{-t}Y(\omega)\)</span>, where <span class="math notranslate nohighlight">\(Y\)</span> is a uniformly distributed random variable in the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<p><strong>Physical Meaning:</strong> Random initial value that decays exponentially over time (see Problem 2).</p>
</div>
</section>
<section id="example-of-the-symmetric-random-walk">
<h2>Example of the Symmetric Random Walk<a class="headerlink" href="#example-of-the-symmetric-random-walk" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example: Symmetric Random Walk</p>
<p>Consider the discrete-time and discrete-state process that we have seen in the first lecture: the symmetric random walk.</p>
<p><strong>Experiment <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>:</strong> A coin is tossed infinitely many times</p>
<p><strong>Time set:</strong> <span class="math notranslate nohighlight">\(\mathcal{T} = \{0, 1, 2, \ldots\}\)</span></p>
<p><strong>Sample space:</strong>
$<span class="math notranslate nohighlight">\(
\Omega = \{\omega = \omega_1 \omega_2 \ldots \omega_n \mid \omega_n \text{ is the outcome of the } n\text{th coin toss}\}
\)</span>$</p>
<p>At each toss, a <strong>head</strong> or a <strong>tail</strong> is obtained with respective probabilities <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(1-p\)</span>.</p>
</div>
<section id="one-step-increment">
<h3>One-Step Increment<a class="headerlink" href="#one-step-increment" title="Link to this heading">#</a></h3>
<p>The one-step increment is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y_i = \begin{cases}
-1 &amp; \text{if } \omega_i = T \\
+1 &amp; \text{if } \omega_i = H
\end{cases}
\end{split}\]</div>
</section>
<section id="random-walk-definition">
<h3>Random Walk Definition<a class="headerlink" href="#random-walk-definition" title="Link to this heading">#</a></h3>
<p>The random walk is defined as:</p>
<div class="math notranslate nohighlight">
\[
X_0 = 0, \quad X_n = \sum_{i=1}^{n} Y_i \quad \text{for all } n \geq 1
\]</div>
<p><strong>Physical Meaning:</strong> Starting at 0, the process moves up or down by 1 at each time step, depending on the coin toss. This is the discrete analog of Brownian motion.</p>
</section>
</section>
<section id="first-and-second-moments">
<h2>First and Second Moments<a class="headerlink" href="#first-and-second-moments" title="Link to this heading">#</a></h2>
<p>Here, we fix the time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>First and Second Moments:</strong></p>
<ul class="simple">
<li><p><strong>First moment (mean):</strong> <span class="math notranslate nohighlight">\(E[X(t)]\)</span></p></li>
<li><p><strong>Second moment (variance):</strong>
$<span class="math notranslate nohighlight">\(
\text{Var}[X(t)] = E[(X(t) - E[X(t)])^2]
\)</span>$</p></li>
</ul>
</div>
<p>Some examples were shown in Chapter 1. One can also compute higher moments in a similar way, provided they exist.</p>
<p><strong>Physical Meaning:</strong> The mean tells us the expected value at time <span class="math notranslate nohighlight">\(t\)</span>, while the variance measures the spread or uncertainty around that expected value.</p>
</section>
<section id="distribution-of-a-stochastic-process">
<h2>Distribution of a Stochastic Process<a class="headerlink" href="#distribution-of-a-stochastic-process" title="Link to this heading">#</a></h2>
<p>A stochastic process can be generally characterized by its distribution.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Single-Time Distribution is Insufficient:</strong></p>
<p>If you simply fix the time <span class="math notranslate nohighlight">\(t \in \mathcal{T}\)</span> and look at the distribution of the random variable <span class="math notranslate nohighlight">\(X(t, \cdot)\)</span> at that fixed time, you <strong>do not capture the time structure</strong> of the stochastic process over time.</p>
</div>
<section id="distribution-function-of-order-k">
<h3>Distribution Function of Order <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#distribution-function-of-order-k" title="Link to this heading">#</a></h3>
<p>In order to characterize a stochastic process, you must look at the <strong>joint distribution</strong> of the process taken at different times.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Distribution Function of Order <span class="math notranslate nohighlight">\(k\)</span></strong></p>
<p>Given an arbitrary set of <span class="math notranslate nohighlight">\(k\)</span> times <span class="math notranslate nohighlight">\(t_1, t_2, \ldots, t_k\)</span>, the joint cumulative distribution function of the random vector <span class="math notranslate nohighlight">\((X(t_1), X(t_2), \ldots, X(t_k))\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
F(x_1, \ldots, x_k; t_1, \ldots, t_k) = P[X(t_1) \leq x_1, \ldots, X(t_k) \leq x_k]
\]</div>
</div>
<p><strong>Physical Meaning:</strong> This captures the joint behavior of the process at multiple time points, which is essential for understanding the dynamics.</p>
</section>
<section id="discrete-and-continuous-cases">
<h3>Discrete and Continuous Cases<a class="headerlink" href="#discrete-and-continuous-cases" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Discrete Case:</strong></p>
<p>In the discrete case, the distribution of order <span class="math notranslate nohighlight">\(k\)</span> can also be described by the <strong>probability mass function</strong>:</p>
<div class="math notranslate nohighlight">
\[
P(x_1, \ldots, x_k; n_1, \ldots, n_k) = P[X_{n_1} = x_1, \ldots, X_{n_k} = x_k]
\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Continuous Case:</strong></p>
<p>In the continuous case, when the joint cumulative distribution function is differentiable <span class="math notranslate nohighlight">\(k\)</span> times, the <strong>joint density function</strong> of order <span class="math notranslate nohighlight">\(k\)</span> is given for a set of <span class="math notranslate nohighlight">\(k\)</span> times, <span class="math notranslate nohighlight">\(t_1, t_2, \ldots, t_k\)</span>, by:</p>
<div class="math notranslate nohighlight">
\[
f(x_1, \ldots, x_k; t_1, \ldots, t_k) = \frac{\partial^k}{\partial x_1 \cdots \partial x_k} F(x_1, \ldots, x_k; t_1, \ldots, t_k)
\]</div>
</div>
</section>
</section>
<section id="independence-and-stationarity-of-increments">
<h2>Independence and Stationarity of Increments<a class="headerlink" href="#independence-and-stationarity-of-increments" title="Link to this heading">#</a></h2>
<p>In financial models, we use as building blocks processes with independent and stationary increments, such as the one-step increments of the random walk.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Independent Increments</strong></p>
<p>If, for all <span class="math notranslate nohighlight">\(0 \leq t_1 &lt; t_2 &lt; t_3 &lt; \cdots &lt; t_n\)</span>, the random variables:</p>
<div class="math notranslate nohighlight">
\[
X(t_2) - X(t_1), \quad X(t_3) - X(t_2), \quad \ldots, \quad X(t_n) - X(t_{n-1})
\]</div>
<p>are independent, then <span class="math notranslate nohighlight">\(X\)</span> is said to be a process with <strong>independent increments</strong>.</p>
</div>
<p><strong>Physical Meaning:</strong> What happens in one time interval doesn’t affect what happens in a non-overlapping time interval. This is crucial for modeling unpredictable markets.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Stationary Increments</strong></p>
<p>If, for all <span class="math notranslate nohighlight">\(0 \leq t_1 &lt; t_2\)</span>, for all <span class="math notranslate nohighlight">\(s \geq 0\)</span>, the random variables:</p>
<div class="math notranslate nohighlight">
\[
X(t_2) - X(t_1) \quad \text{and} \quad X(t_2+s) - X(t_1+s)
\]</div>
<p>have the same distribution, <span class="math notranslate nohighlight">\(X\)</span> is said to be a process with <strong>stationary increments</strong>.</p>
</div>
<p><strong>Physical Meaning:</strong> The statistical properties of changes in the process depend only on the length of the time interval, not on when the interval starts. This is time-translation invariance.</p>
</section>
<section id="an-example-the-bernoulli-process-revisited">
<h2>An Example: The Bernoulli Process Revisited<a class="headerlink" href="#an-example-the-bernoulli-process-revisited" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example: Rolling a Die</p>
<p><strong>Bernoulli trials:</strong> One rolls a die independently an infinite number of times and defines a <strong>success</strong> as rolling a six.</p>
<p><strong>Bernoulli process:</strong> A sequence of Bernoulli random variables associated with Bernoulli trials <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X_k = \begin{cases}
1 &amp; \text{if } k\text{th trial is a success} \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
</div>
<section id="distribution-and-expectation">
<h3>Distribution and Expectation<a class="headerlink" href="#distribution-and-expectation" title="Link to this heading">#</a></h3>
<p>The Bernoulli variable <span class="math notranslate nohighlight">\(X_k\)</span> has a Bernoulli distribution:</p>
<div class="math notranslate nohighlight">
\[
p(x) = p^x (1-p)^{1-x}, \quad \text{for } x = 0, 1
\]</div>
<p>with parameter <span class="math notranslate nohighlight">\(p = 1/6\)</span>.</p>
<p>In addition, its expectation is given by:</p>
<div class="math notranslate nohighlight">
\[
E[X_k] = \text{probability of success} \cdot 1 = \frac{1}{6}
\]</div>
</section>
<section id="cumulative-successes">
<h3>Cumulative Successes<a class="headerlink" href="#cumulative-successes" title="Link to this heading">#</a></h3>
<p>One may also define the stochastic process <span class="math notranslate nohighlight">\(Y_n\)</span> representing the <strong>number of successes after <span class="math notranslate nohighlight">\(n\)</span> trials</strong>:</p>
<div class="math notranslate nohighlight">
\[
Y_n = \sum_{k=1}^{n} X_k
\]</div>
<p>where <span class="math notranslate nohighlight">\(X_k\)</span> are the Bernoulli random variables.</p>
<p>The distribution of <span class="math notranslate nohighlight">\(Y_n\)</span> is <strong>Binomial</strong> with parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>By inverting this formula, we also obtain the relationship:</p>
<div class="math notranslate nohighlight">
\[
X_{n+1} = Y_{n+1} - Y_n
\]</div>
<p>The process <span class="math notranslate nohighlight">\(X_{n+1} = Y_{n+1} - Y_n\)</span> is called the <strong>one-step increment</strong> of the process <span class="math notranslate nohighlight">\(Y_n\)</span>.</p>
</section>
<section id="independence-and-stationarity">
<h3>Independence and Stationarity<a class="headerlink" href="#independence-and-stationarity" title="Link to this heading">#</a></h3>
<p>Furthermore, since <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent random variables, the process <span class="math notranslate nohighlight">\(Y_n\)</span> has <strong>independent increments</strong>.</p>
<p>Besides, since <span class="math notranslate nohighlight">\(Y_{k+n+1} - Y_{k+n} = X_{k+n+1}\)</span> and <span class="math notranslate nohighlight">\(Y_{n+1} - Y_n = X_{n+1}\)</span> have the same distribution, the process <span class="math notranslate nohighlight">\(Y_n\)</span> also has <strong>stationary increments</strong>.</p>
<p><strong>Physical Meaning:</strong> Each trial is independent of previous trials, and the probability structure doesn’t change over time.</p>
</section>
</section>
<section id="another-important-example-the-symmetric-random-walk">
<h2>Another Important Example: The Symmetric Random Walk<a class="headerlink" href="#another-important-example-the-symmetric-random-walk" title="Link to this heading">#</a></h2>
<p>Given a set of integers <span class="math notranslate nohighlight">\(0 = k_0 &lt; k_1 &lt; \cdots &lt; k_i &lt; k_{i+1} &lt; \cdots &lt; k_m\)</span>, we define the increments of the random walk:</p>
<div class="math notranslate nohighlight">
\[
X_{k_{i+1}} - X_{k_i} = \sum_{j=k_i+1}^{k_{i+1}} Y_j
\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Properties of Random Walk:</strong></p>
<p>The increments:
$<span class="math notranslate nohighlight">\(
X_{k_1} - X_0, \quad X_{k_2} - X_{k_1}, \quad \ldots, \quad X_{k_i+1} - X_{k_i}, \quad \ldots, \quad X_{k_m} - X_{k_{m-1}}
\)</span>$</p>
<p>are <strong>independent</strong> random variables. So we can state that the random walk has <strong>independent increments</strong>.</p>
<p>Clearly, the increments of the random walk are also <strong>stationary</strong> since the coin tosses are independent and identical.</p>
</div>
<section id="moments-of-increments">
<h3>Moments of Increments<a class="headerlink" href="#moments-of-increments" title="Link to this heading">#</a></h3>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Derivation of Mean and Variance</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Mean of increment:</strong>
$<span class="math notranslate nohighlight">\(
E[X_{k_{i+1}} - X_{k_i}] = \sum_{j=k_i+1}^{k_{i+1}} E[Y_j] = 0
\)</span>$</p>
<p class="sd-card-text"><strong>Variance of increment:</strong>
$<span class="math notranslate nohighlight">\(
\begin{align}
\text{Var}[X_{k_{i+1}} - X_{k_i}] &amp;= E\left[\left(\sum_{j=k_i+1}^{k_{i+1}} Y_j\right)^2\right] \\
&amp;= E\left[\sum_{j=k_i+1}^{k_{i+1}} Y_j^2 + \sum_{j=k_i+1}^{k_{i+1}} \sum_{k \neq j} Y_j Y_k\right] \\
&amp;= \sum_{j=k_i+1}^{k_{i+1}} E[Y_j^2] + \sum_{j=k_i+1}^{k_{i+1}} \sum_{k \neq j} E[Y_j Y_k] \\
&amp;= \sum_{j=k_i+1}^{k_{i+1}} 1 + \sum_{j=k_i+1}^{k_{i+1}} \sum_{k \neq j} 0 \\
&amp;= k_{i+1} - k_i
\end{align}
\)</span>$</p>
</div>
</details><p><strong>Physical Meaning:</strong> The variance of the increment over the time interval <span class="math notranslate nohighlight">\([k_i, k_{i+1}]\)</span> is equal to <span class="math notranslate nohighlight">\(k_{i+1} - k_i\)</span>, the length of the interval. This is characteristic of diffusive processes.</p>
</section>
<section id="overall-mean-and-variance">
<h3>Overall Mean and Variance<a class="headerlink" href="#overall-mean-and-variance" title="Link to this heading">#</a></h3>
<p>We also deduce the expectation and variance of the random walk from the last two results:</p>
<div class="math notranslate nohighlight">
\[
E[X_k \mid X_0 = 0] = E[X_k - X_0 \mid X_0 = 0] = E[X_k - X_0] = 0
\]</div>
<div class="math notranslate nohighlight">
\[
\text{Var}[X_k] = \text{Var}[X_k - X_0] = k
\]</div>
<p><strong>Physical Meaning:</strong> The random walk has zero drift and variance that grows linearly with time, consistent with diffusive behavior.</p>
</section>
</section>
<section id="autocovariance-and-autocorrelation-functions">
<h2>Autocovariance and Autocorrelation Functions<a class="headerlink" href="#autocovariance-and-autocorrelation-functions" title="Link to this heading">#</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Autocovariance Function</strong></p>
<p>At the point <span class="math notranslate nohighlight">\((t_1, t_2)\)</span>, it is defined by:</p>
<div class="math notranslate nohighlight">
\[
C_X(t_1, t_2) = E[X(t_1)X(t_2)] - E[X(t_1)]E[X(t_2)]
\]</div>
<p>and can also be written as:</p>
<div class="math notranslate nohighlight">
\[
C_X(t_1, t_2) = E[(X(t_1) - E[X(t_1)])(X(t_2) - E[X(t_2)])]
\]</div>
</div>
<p><strong>Physical Meaning:</strong> The autocovariance measures how the process at time <span class="math notranslate nohighlight">\(t_1\)</span> is related to the process at time <span class="math notranslate nohighlight">\(t_2\)</span>. Positive values indicate they tend to move together.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Autocorrelation Function</strong></p>
<p>At the point <span class="math notranslate nohighlight">\((t_1, t_2)\)</span>, it is given by:</p>
<div class="math notranslate nohighlight">
\[
\rho_X(t_1, t_2) = \frac{C_X(t_1, t_2)}{(\text{Var}[X(t_1)] \text{Var}[X(t_2)])^{1/2}}
\]</div>
</div>
<p><strong>Physical Meaning:</strong> The autocorrelation is a normalized version of autocovariance, always between -1 and 1, making it easier to interpret the strength of the relationship.</p>
</section>
<section id="example-of-the-ar-1-process-for-a-1">
<h2>Example of the AR(1) Process for <span class="math notranslate nohighlight">\(a = 1\)</span><a class="headerlink" href="#example-of-the-ar-1-process-for-a-1" title="Link to this heading">#</a></h2>
<p>We revisit the generalized random walk that was introduced in Chapter 1.</p>
<p>Consider the process <span class="math notranslate nohighlight">\(Y_n\)</span> at time <span class="math notranslate nohighlight">\(n\)</span>, whose evolution is defined as:</p>
<div class="math notranslate nohighlight">
\[
Y_{n+1} = Y_n + \epsilon_{n+1}
\]</div>
<p>where the random variables <span class="math notranslate nohighlight">\(\epsilon_1, \epsilon_2, \ldots, \epsilon_{n+1}, \ldots\)</span> are independent and identically distributed.</p>
<p>We also assume that the variables <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are centered and we denote their common variance by <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>By using successive iterations, one can also rewrite this model as:</p>
<div class="math notranslate nohighlight">
\[
Y_{n+1} = Y_0 + \sum_{i=1}^{n+1} \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y_0\)</span> denotes the initial condition at time <span class="math notranslate nohighlight">\(n = 0\)</span>.</p>
<section id="computing-the-autocovariance-function">
<h3>Computing the Autocovariance Function<a class="headerlink" href="#computing-the-autocovariance-function" title="Link to this heading">#</a></h3>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Derivation of Autocovariance</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
E[Y_{n+p} Y_n] &amp;= E\left[\left(Y_0 + \sum_{i=1}^{n+p} \epsilon_i\right)\left(Y_0 + \sum_{i=1}^{n} \epsilon_i\right)\right] \\
&amp;= Y_0^2 + \sum_{i=1}^{n+p} \sum_{j=1}^{n} E[\epsilon_i \epsilon_j] + Y_0 \left(\sum_{i=1}^{n+p} E[\epsilon_i] + \sum_{i=1}^{n} E[\epsilon_i]\right) \\
&amp;= Y_0^2 + \sum_{i=1}^{n} E[\epsilon_i^2] + \sum_{i=1}^{n+p} \sum_{j=1, j \neq i}^{n} E[\epsilon_i]E[\epsilon_j] + 0 \\
&amp;= Y_0^2 + n\sigma^2 + 0 \\
&amp;= Y_0^2 + n\sigma^2
\end{align}
\end{split}\]</div>
<p class="sd-card-text">Subtracting the product of the expectations, we find that the autocovariance function is given by:</p>
<div class="math notranslate nohighlight">
\[
C_X(n+p, n) = n\sigma^2
\]</div>
</div>
</details><p><strong>Physical Meaning:</strong> The autocovariance depends on the smaller of the two time indices, reflecting the cumulative variance up to that point.</p>
</section>
</section>
<section id="strict-sense-and-wide-sense-stationarity">
<h2>Strict Sense and Wide Sense Stationarity<a class="headerlink" href="#strict-sense-and-wide-sense-stationarity" title="Link to this heading">#</a></h2>
<p>We write two definitions of stationarity that are applicable to any process (not just its increments).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Strict Sense Stationary (SSS)</strong></p>
<p><span class="math notranslate nohighlight">\(X\)</span> is <strong>strict sense stationary</strong> if for all <span class="math notranslate nohighlight">\(s, n, t_1, \ldots, t_n, x_1, \ldots, x_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
F(x_1, \ldots, x_n; t_1, \ldots, t_n) = F(x_1, \ldots, x_n; t_1+s, \ldots, t_n+s)
\]</div>
</div>
<p><strong>Physical Meaning:</strong> The distribution of the stationary process is independent of a time shift. The statistical properties don’t change over time, though the process itself may still vary.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Wide Sense Stationary (WSS)</strong></p>
<p><span class="math notranslate nohighlight">\(X\)</span> is <strong>wide sense stationary</strong> if:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(E[X(t)]\)</span> is independent of <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R_X(t, t+h) = E[X(t)X(t+h)]\)</span> depends only on <span class="math notranslate nohighlight">\(h\)</span> and not on <span class="math notranslate nohighlight">\(t\)</span></p></li>
</ol>
</div>
<p><strong>Physical Meaning:</strong> WSS is a weaker condition than SSS, requiring only that the mean is constant and the autocovariance depends only on the time lag, not absolute time.</p>
<section id="relationship-between-sss-and-wss">
<h3>Relationship Between SSS and WSS<a class="headerlink" href="#relationship-between-sss-and-wss" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>SSS Implies WSS:</strong></p>
<p>If the process <span class="math notranslate nohighlight">\(X\)</span> is strict stationary, it will also satisfy the wide-sense stationarity definition.</p>
<p><strong>Proof:</strong></p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> is strict stationary, we have that for all <span class="math notranslate nohighlight">\(t \geq 0\)</span>:
$<span class="math notranslate nohighlight">\(
E[X(t)] = E[X(0)]
\)</span>$</p></li>
<li><p>By strict stationarity of <span class="math notranslate nohighlight">\(X\)</span>:
$<span class="math notranslate nohighlight">\(
E[X(t)X(t+h)] = E[X(0)X(h)]
\)</span><span class="math notranslate nohighlight">\(
which only depends on \)</span>h$.</p></li>
<li><p>Finally, if <span class="math notranslate nohighlight">\(X\)</span> is strict stationary, its variance does not depend on time:
$<span class="math notranslate nohighlight">\(
\text{var}(X(t)) = E[X(t)X(t)] - E[X(t)]^2 = E[X(0)^2] - E[X(0)]^2
\)</span>$</p></li>
</ol>
</div>
</section>
</section>
<section id="examples-of-stationarity">
<h2>Examples of Stationarity<a class="headerlink" href="#examples-of-stationarity" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example 1: Time-Independent Process</p>
<p>Let <span class="math notranslate nohighlight">\(X(t) = Y\)</span> for <span class="math notranslate nohighlight">\(t \geq 0\)</span> where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable.</p>
<p><strong>Conclusion:</strong> Clearly, <span class="math notranslate nohighlight">\(X\)</span> is SSS because its distribution does not depend on the time variable at all.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 2: Bernoulli Process</p>
<p>The Bernoulli process described earlier (number of successes) is <strong>not WSS</strong> because its expectation is not independent of the time <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>Indeed, <span class="math notranslate nohighlight">\(E[Y_n] = np\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example 3: Linear Growth Process</p>
<p>The process <span class="math notranslate nohighlight">\(X(t, \cdot) = tY(\cdot)\)</span> where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable is <strong>not WSS</strong> since:</p>
<div class="math notranslate nohighlight">
\[
E[X(t)] = tE[Y]
\]</div>
<p>is not independent of <span class="math notranslate nohighlight">\(t\)</span>, except in the special case where <span class="math notranslate nohighlight">\(E[Y] = 0\)</span>.</p>
</div>
<section id="example-4-the-symmetric-random-walk">
<h3>Example 4: The Symmetric Random Walk<a class="headerlink" href="#example-4-the-symmetric-random-walk" title="Link to this heading">#</a></h3>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Checking WSS for Random Walk</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Mean:</strong>
Firstly, we have seen earlier that, for all <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(E[X_k] = 0\)</span>. So the random walk <span class="math notranslate nohighlight">\((X_k)_k\)</span> is independent of the time index <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p class="sd-card-text"><strong>Autocovariance:</strong>
Next, we need to compute:
$<span class="math notranslate nohighlight">\(
\begin{align}
R_X(k, k+j) &amp;= E[X_k X_{k+j}] \\
&amp;= E[X_k(X_{k+j} - X_k) + X_k^2] \\
&amp;= E[X_k]E[X_{k+j} - X_k] + E[X_k^2] \\
&amp;= 0 + E[X_k^2] \\
&amp;= k
\end{align}
\)</span>$</p>
<p class="sd-card-text"><strong>Conclusion:</strong> The symmetric random walk is <strong>not WSS</strong> since <span class="math notranslate nohighlight">\(R_X(k, k+j)\)</span> depends on the time variable <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
</details><p><strong>Physical Meaning:</strong> Even though the random walk has constant mean, its autocovariance structure changes with absolute time, making it non-stationary.</p>
</section>
</section>
<section id="the-ar-1-process">
<h2>The AR(1) Process<a class="headerlink" href="#the-ar-1-process" title="Link to this heading">#</a></h2>
<p>We denote the AR(1) process at time <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(X_n\)</span>. We assume that it follows the evolution:</p>
<div class="math notranslate nohighlight">
\[
X_{n+1} = aX_n + \epsilon_{n+1}
\]</div>
<p>where the random variables <span class="math notranslate nohighlight">\(\epsilon_1, \epsilon_2, \ldots, \epsilon_n, \ldots\)</span> are independent and identically distributed.</p>
<p>We assume that they are centered and denote their common variance by <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<section id="explicit-form">
<h3>Explicit Form<a class="headerlink" href="#explicit-form" title="Link to this heading">#</a></h3>
<p>Given an initial condition <span class="math notranslate nohighlight">\(X_0\)</span>, we can rewrite <span class="math notranslate nohighlight">\(X_n\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
X_{n+1} &amp;= aX_n + \epsilon_{n+1} \tag{1} \\
&amp;= a^2 X_{n-1} + \epsilon_{n+1} + a\epsilon_n \tag{2} \\
&amp;= a^{n+1}X_0 + \sum_{i=0}^{n} a^{n+1-i} \epsilon_i \tag{3}
\end{align}
\end{split}\]</div>
</section>
<section id="expectation">
<h3>Expectation<a class="headerlink" href="#expectation" title="Link to this heading">#</a></h3>
<p>By using the fact that the variables <span class="math notranslate nohighlight">\(\epsilon_n\)</span> are centered, we can obtain the expectation of <span class="math notranslate nohighlight">\(X_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
E[X_{n+1}] = a^{n+1} X_0
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>WSS Analysis:</strong></p>
<p>In general, the process is <strong>not WSS</strong> because its expectation depends on <span class="math notranslate nohighlight">\(n\)</span>, unless <span class="math notranslate nohighlight">\(X_0 = 0\)</span>.</p>
<p><strong>Long-run behavior:</strong></p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(|a| &lt; 1\)</span>: <span class="math notranslate nohighlight">\(E[X_{n+1}] \to 0\)</span> as <span class="math notranslate nohighlight">\(n \to +\infty\)</span></p>
<ul>
<li><p>0 is the <strong>long-run mean</strong> of the process</p></li>
</ul>
</li>
<li><p>When <span class="math notranslate nohighlight">\(|a| &gt; 1\)</span>: the mean explodes and does not converge to a constant</p>
<ul>
<li><p>Precise behavior depends on the sign of <span class="math notranslate nohighlight">\(X_0\)</span> and of <span class="math notranslate nohighlight">\(a\)</span></p></li>
</ul>
</li>
</ul>
</div>
<p><strong>Physical Meaning:</strong> For <span class="math notranslate nohighlight">\(|a| &lt; 1\)</span>, the process is mean-reverting toward zero. For <span class="math notranslate nohighlight">\(|a| &gt; 1\)</span>, it’s explosive. The case <span class="math notranslate nohighlight">\(|a| = 1\)</span> is a random walk (unit root).</p>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h3>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Derivation of Variance</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\text{var}[X_{n+1}] &amp;= \sum_{i=0}^{n} a^{2i} \text{var}[\epsilon_i] \\
&amp;= \sigma^2 \sum_{i=0}^{n} a^{2i} \\
&amp;= \sigma^2 \frac{1 - a^{2(n+1)}}{1 - a^2}
\end{align}
\end{split}\]</div>
</div>
</details><div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Long-Run Variance:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(|a| &lt; 1\)</span>: <span class="math notranslate nohighlight">\(\text{var}[X_{n+1}] \to \frac{\sigma^2}{1-a^2}\)</span> as <span class="math notranslate nohighlight">\(n \to +\infty\)</span></p>
<ul>
<li><p>This result suggests that the AR(1) model is stationary in the long run when <span class="math notranslate nohighlight">\(|a| &lt; 1\)</span></p></li>
</ul>
</li>
<li><p>If <span class="math notranslate nohighlight">\(|a| &gt; 1\)</span>: <span class="math notranslate nohighlight">\(\text{var}[X_{n+1}] \to +\infty\)</span></p>
<ul>
<li><p>The variance of the random walk is explosive in that case</p></li>
</ul>
</li>
</ul>
</div>
<p><strong>Physical Meaning:</strong> When <span class="math notranslate nohighlight">\(|a| &lt; 1\)</span>, the process settles into a stationary distribution with finite variance. When <span class="math notranslate nohighlight">\(|a| \geq 1\)</span>, variance grows without bound.</p>
</section>
</section>
<section id="the-gaussian-processes">
<h2>The Gaussian Processes<a class="headerlink" href="#the-gaussian-processes" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example: Wiener Process</p>
<p>We will study later the <strong>Wiener process</strong> (also called the Brownian motion), which is a Gaussian process.</p>
</div>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Gaussian Process</strong></p>
<p>A stochastic process <span class="math notranslate nohighlight">\(X\)</span> is said to be a <strong>Gaussian Process</strong> if the vector:</p>
<div class="math notranslate nohighlight">
\[
(X(t_1), X(t_2), \ldots, X(t_n))
\]</div>
<p>has a multi-normal distribution for any <span class="math notranslate nohighlight">\(n\)</span> and for all <span class="math notranslate nohighlight">\(t_1, t_2, \ldots, t_n\)</span>.</p>
</div>
<p><strong>Physical Meaning:</strong> At any finite collection of times, the joint distribution is multivariate normal. This makes analysis tractable while still capturing complex dependencies.</p>
</section>
<section id="multi-normal-distribution">
<h3>Multi-Normal Distribution<a class="headerlink" href="#multi-normal-distribution" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Multi-Normal Distribution</strong></p>
<p>A random vector <span class="math notranslate nohighlight">\((X_1, \ldots, X_n)\)</span> has a multi-normal distribution if each random variable <span class="math notranslate nohighlight">\(X_k\)</span> can be expressed as a linear combination of <span class="math notranslate nohighlight">\(m\)</span> independent standard Gaussian random variables with <span class="math notranslate nohighlight">\(m \leq n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
X_k = \bar{X}_k + \sum_{j=1}^{m} c_{kj} Z_j
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{X}_k \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(Z_j \sim \mathcal{N}(0,1)\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Key Properties:</strong></p>
<p>An immediate consequence of this definition is that any <strong>affine combination</strong> of multi-normally distributed random vectors has a multi-normal distribution.</p>
<p>Furthermore:
$<span class="math notranslate nohighlight">\(
\begin{align}
E[X_k] &amp;= \bar{X}_k \\
\text{Var}[X_k] &amp;= \sum_{j=1}^{m} c_{kj}^2 \\
\text{Cov}[X_i, X_k] &amp;= \sum_{l=1}^{m} c_{il} c_{kl}
\end{align}
\)</span>$</p>
</div>
</section>
<section id="joint-distribution">
<h3>Joint Distribution<a class="headerlink" href="#joint-distribution" title="Link to this heading">#</a></h3>
<p>The joint distribution of the vector <span class="math notranslate nohighlight">\((X_1, X_2, \ldots, X_n)\)</span> is completely determined by the <strong>vector of means</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = (\bar{X}_1, \ldots, \bar{X}_n)\)</span> and the <strong>covariance matrix</strong> <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \begin{bmatrix}
\text{Var}[X_1] &amp; \text{Cov}[X_1, X_2] &amp; \cdots &amp; \text{Cov}[X_1, X_n] \\
\text{Cov}[X_1, X_2] &amp; \text{Var}[X_2] &amp; \cdots &amp; \text{Cov}[X_2, X_n] \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\text{Cov}[X_1, X_n] &amp; \cdots &amp; \cdots &amp; \text{Var}[X_n]
\end{bmatrix}
\end{split}\]</div>
<p>Note that since <span class="math notranslate nohighlight">\(\text{Cov}[X_i, X_j] = \text{Cov}[X_j, X_i]\)</span>, the matrix <span class="math notranslate nohighlight">\(K\)</span> is <strong>symmetric</strong>.</p>
</section>
<section id="density-function">
<h3>Density Function<a class="headerlink" href="#density-function" title="Link to this heading">#</a></h3>
<p>If <span class="math notranslate nohighlight">\(K\)</span> is nonsingular, the density of <span class="math notranslate nohighlight">\((X_1, X_2, \ldots, X_n)\)</span> exists and can be written as:</p>
<div class="math notranslate nohighlight">
\[
f_X(x) = \frac{1}{(2\pi)^{n/2}} \frac{1}{(\det K)^{1/2}} \exp\left\{-\frac{1}{2}(x - \boldsymbol{\mu})^T K^{-1} (x - \boldsymbol{\mu})\right\}
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Singular Covariance Matrix:</strong></p>
<p>If <span class="math notranslate nohighlight">\(K\)</span> is singular, the vector of random variables <span class="math notranslate nohighlight">\((X_1, \ldots, X_n)\)</span> does not even have a density function!</p>
</div>
</section>
<section id="properties-of-covariance-matrix">
<h3>Properties of Covariance Matrix<a class="headerlink" href="#properties-of-covariance-matrix" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(K\)</span> is also <strong>nonnegative definite</strong> in the following sense:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_i \lambda_j \text{Cov}[X_i, X_j] \geq 0 \quad \text{for all } \boldsymbol{\lambda} = (\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n
\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Independence Condition:</strong></p>
<p>In the case when <span class="math notranslate nohighlight">\((X_1, X_2, \ldots, X_n)\)</span> has a multi-normal distribution, the random variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are <strong>independent if and only if</strong> the matrix <span class="math notranslate nohighlight">\(K\)</span> is a diagonal matrix.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>General Caveat:</strong></p>
<p>In general, zero linear correlations do not imply independence! The random variables could depend on one another in a nonlinear way.</p>
</div>
<p>We recommend Jacod and Protter [1] for a review of the multi-normal distribution and Stefanica [4] for a review of linear algebra.</p>
</section>
<section id="gaussian-processes-and-stationarity">
<h3>Gaussian Processes and Stationarity<a class="headerlink" href="#gaussian-processes-and-stationarity" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Theorem: WSS Implies SSS for Gaussian Processes</strong></p>
<p>If a Gaussian process is weak sense stationary, then it is also strict sense stationary.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Proof</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">We consider a Gaussian process <span class="math notranslate nohighlight">\(X\)</span> and suppose that it is weak sense stationary. Then, we can write that:</p>
<div class="math notranslate nohighlight">
\[
R_X(t_1, t_2) = R_X(t_1 - t_2)
\]</div>
<p class="sd-card-text">and:</p>
<div class="math notranslate nohighlight">
\[
m_X(t) = \mu
\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\mu\)</span> is a constant since the expectation of <span class="math notranslate nohighlight">\(X\)</span> does not depend on <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p class="sd-card-text">Furthermore, we know that the joint distribution of <span class="math notranslate nohighlight">\(X\)</span> taken at different times is uniquely characterized by <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and:</p>
<div class="math notranslate nohighlight">
\[
C_X(t_1, t_2) = R_X(t_1 - t_2) - \mu^2
\]</div>
<p class="sd-card-text">and hence is invariant with respect to any translation in the time variable <span class="math notranslate nohighlight">\(t + s\)</span>.</p>
</div>
</details><p><strong>Physical Meaning:</strong> For Gaussian processes, second-order properties (mean and covariance) completely determine the distribution, so WSS automatically implies full stationarity.</p>
</section>
</section>
<section id="ergodicity">
<h2>Ergodicity<a class="headerlink" href="#ergodicity" title="Link to this heading">#</a></h2>
<p>Generally, a process is said to be <strong>ergodic</strong> if any of its characteristics can be obtained from observing a single sample path in the long run.</p>
<p>A stochastic process can be:</p>
<ul class="simple">
<li><p><strong>Mean-ergodic</strong></p></li>
<li><p><strong>Distribution-ergodic</strong></p></li>
<li><p><strong>Autocorrelation function-ergodic</strong></p></li>
<li><p>etc.</p></li>
</ul>
<p>In this lecture, we focus on <strong>mean-ergodicity</strong>.</p>
<section id="temporal-mean">
<h3>Temporal Mean<a class="headerlink" href="#temporal-mean" title="Link to this heading">#</a></h3>
<p>First of all, we define the <strong>temporal mean</strong> of the process <span class="math notranslate nohighlight">\(X\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\langle X \rangle_{\tau} = \frac{1}{\tau} \int_0^{\tau} X(t, \omega) \, dt
\]</div>
<p>Next, we assume that the expectation of the process <span class="math notranslate nohighlight">\(X\)</span> is independent of time and we let <span class="math notranslate nohighlight">\(m = E[X(t)]\)</span> be the constant mean of <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
<section id="mean-ergodic-definition">
<h3>Mean-Ergodic Definition<a class="headerlink" href="#mean-ergodic-definition" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Definition: Mean-Ergodic</strong></p>
<p>A process <span class="math notranslate nohighlight">\(X\)</span> with constant mean <span class="math notranslate nohighlight">\(m\)</span> is said to be <strong>mean-ergodic</strong> if:</p>
<div class="math notranslate nohighlight">
\[
\lim_{\tau \to +\infty} \text{var}[\langle X \rangle_{\tau}] = 0
\]</div>
</div>
<p>Since:</p>
<div class="math notranslate nohighlight">
\[
E[\langle X \rangle_{\tau}] = \frac{1}{\tau} \int_0^{\tau} E[X(t)] \, dt = m
\]</div>
<p>the above definition is equivalent to the convergence of the temporal mean to <span class="math notranslate nohighlight">\(m\)</span> in mean-squares as <span class="math notranslate nohighlight">\(\tau \to +\infty\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Equivalent Definition:</strong></p>
<p>A process <span class="math notranslate nohighlight">\(X\)</span> with constant mean <span class="math notranslate nohighlight">\(m\)</span> is said to be mean-ergodic if:</p>
<div class="math notranslate nohighlight">
\[
\lim_{\tau \to +\infty} E[(\langle X \rangle_{\tau} - m)^2] = 0
\]</div>
</div>
<p><strong>Physical Meaning:</strong> For an ergodic process, you can estimate the ensemble average (expected value over all outcomes) by computing a time average along a single sample path. This is crucial for empirical analysis where we only observe one realization.</p>
</section>
</section>
<section id="examples-of-ergodicity">
<h2>Examples of Ergodicity<a class="headerlink" href="#examples-of-ergodicity" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Example 1: Time-Independent Process (Not Ergodic)</p>
<p>Let <span class="math notranslate nohighlight">\(X(t) = Y\)</span>, where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable, for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><span class="math notranslate nohighlight">\(X\)</span> is <strong>not mean-ergodic</strong>.</p>
<p>Indeed:
$<span class="math notranslate nohighlight">\(
E[X(t)] = E[Y]
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\langle X \rangle_s = \frac{1}{s} \int_0^s X(u) \, du = \frac{1}{s} \int_0^s Y \, du = Y
\]</div>
<p>Consequently:
$<span class="math notranslate nohighlight">\(
\text{var}[\langle X \rangle_s] = \text{var}[Y] \neq 0
\)</span>$</p>
</div>
<p><strong>Physical Meaning:</strong> The time average equals the random variable <span class="math notranslate nohighlight">\(Y\)</span> itself, not its expected value. Different sample paths give different time averages.</p>
<div class="tip admonition">
<p class="admonition-title">Example 2: Bernoulli Variables (Ergodic)</p>
<p>The Bernoulli variables introduced in Chapter 1 are a <strong>mean-ergodic process</strong>.</p>
<p>In a discrete-time setting, the integral must be replaced by a discrete sum:</p>
<div class="math notranslate nohighlight">
\[
\langle X \rangle_N = \frac{1}{N+1} \sum_{n=0}^{N} X(n, \omega)
\]</div>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Verification of Ergodicity</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">In the case of the Bernoulli variables:</p>
<div class="math notranslate nohighlight">
\[
E[\langle X \rangle_k] = E\left[\frac{1}{k} \sum_{i=1}^{k} X_i\right] = \frac{1}{6} = E[X_i]
\]</div>
<p class="sd-card-text">and:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\lim_{k \to +\infty} \text{var}[\langle X \rangle_k] &amp;= \lim_{k \to +\infty} \frac{1}{k^2} \sum_{i=1}^{k} \text{var}[X_i] \\
&amp;= \lim_{k \to +\infty} \frac{1}{k^2} k\left(\frac{1}{6} - \frac{1}{36}\right) \\
&amp;= \lim_{k \to +\infty} \frac{1}{k} \left(\frac{1}{6} - \frac{1}{36}\right) \\
&amp;= 0
\end{align}
\end{split}\]</div>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Example 3: One-Step Increment of Random Walk (Ergodic)</p>
<p>The one-step increment <span class="math notranslate nohighlight">\((k+1, \omega) \mapsto Z_{k+1}(\omega) = X_{k+1}(\omega) - X_k(\omega)\)</span> of the symmetric random walk is a <strong>mean-ergodic process</strong> with mean 0.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Verification for Random Walk Increment</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Indeed, computing the temporal mean of <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle Z \rangle_k = \frac{1}{k+1} \sum_{i=0}^{k} (X_{i+1} - X_i) = \frac{1}{k+1}(X_{k+1} - X_0) = \frac{X_{k+1}}{k+1}
\]</div>
<p class="sd-card-text">Hence:
$<span class="math notranslate nohighlight">\(
E[\langle Z \rangle_k] = 0
\)</span>$</p>
<p class="sd-card-text">and, similarly as in the previous example:</p>
<div class="math notranslate nohighlight">
\[
\lim_{k \to +\infty} \text{var}[\langle Z \rangle_k] = \lim_{k \to +\infty} \frac{1}{(k+1)^2} (k+1) = 0
\]</div>
</div>
</details></section>
<section id="the-features-of-real-financial-data">
<h2>The Features of Real Financial Data<a class="headerlink" href="#the-features-of-real-financial-data" title="Link to this heading">#</a></h2>
<p>This section summarizes some of the findings of the published article by Rama Cont [3].</p>
<section id="log-returns">
<h3>Log Returns<a class="headerlink" href="#log-returns" title="Link to this heading">#</a></h3>
<p>What we have in mind here is the study of the characteristics of the evolution of the <strong>log return</strong> of a financial asset, where, if <span class="math notranslate nohighlight">\(S(t)\)</span> denotes the price of the asset, the log return over the time interval <span class="math notranslate nohighlight">\([t, t+\Delta t]\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
r(t) = \log S(t + \Delta t) - \log S(t)
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Time Scale Dependence:</strong></p>
<p>The properties of the time series <span class="math notranslate nohighlight">\(r(t)\)</span> depend on the <strong>time scale</strong> considered, that is, the exact value of <span class="math notranslate nohighlight">\(\Delta t\)</span>.</p>
<p>In particular, <strong>high-frequency data</strong> reflects the microstructure of financial markets and possesses different characteristics than data at lower frequencies.</p>
</div>
<p>However, researchers have established some common features shared by a wide range of financial assets across a range of time scales, excluding small intraday time scales.</p>
</section>
<section id="stylized-facts">
<h3>Stylized Facts<a class="headerlink" href="#stylized-facts" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 1: Near-Zero Linear Autocorrelations</strong></p>
<p>The linear autocorrelations are typically <strong>almost zero</strong>.</p>
<p><strong>Note:</strong> This is no longer true for small intraday time scales (&lt; 20 minutes).</p>
<p><strong>Important:</strong> The fact that the linear autocorrelation <span class="math notranslate nohighlight">\(\rho_X(t, t+\tau)\)</span>, where <span class="math notranslate nohighlight">\(\tau\)</span> denotes the time lag, is zero does <strong>not</strong> imply the independence of the log returns <span class="math notranslate nohighlight">\(r(t)\)</span> and <span class="math notranslate nohighlight">\(r(t+\tau)\)</span>!</p>
</div>
<p><strong>Physical Meaning:</strong> Past returns don’t predict future returns in a linear way, consistent with weak-form market efficiency. However, nonlinear dependencies may still exist.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 2: Fat Tails</strong></p>
<p>The distribution of returns has <strong>fat tails</strong>, which means that very brutal market events have a non-negligible probability of happening.</p>
<p>The tail behavior is <strong>power-law or Pareto-like</strong>.</p>
</div>
<p>For instance, the cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span> of a Pareto distribution is parametrized by <span class="math notranslate nohighlight">\(\alpha\)</span>, which is known as the <strong>tail index</strong>, and also by <span class="math notranslate nohighlight">\(x_m\)</span>, which is the mode:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F(x) = \begin{cases}
1 - \left(\frac{x_m}{x}\right)^\alpha &amp; x \geq x_m \\
0 &amp; x &lt; x_m
\end{cases}
\end{split}\]</div>
<p><strong>Physical Meaning:</strong> Extreme events (crashes, spikes) are much more likely than the normal distribution would predict. This has major implications for risk management.</p>
</section>
<section id="tail-index">
<h3>Tail Index<a class="headerlink" href="#tail-index" title="Link to this heading">#</a></h3>
<p>Generally, the <strong>tail index</strong> <span class="math notranslate nohighlight">\(\alpha\)</span> of the distribution, which measures how thick the tail is, is defined as the highest absolute moment which is finite.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Tail Index Values:</strong></p>
<ul class="simple">
<li><p>For both the Gaussian and exponential distributions: <span class="math notranslate nohighlight">\(\alpha = +\infty\)</span> (all moments are finite)</p></li>
<li><p>For a power-law distribution: the tail index is the exponent of the distribution</p></li>
<li><p>The tail index can be measured using statistical methods</p></li>
</ul>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Empirical Evidence:</strong></p>
<p>In practice, simple studies show that US and French equities as well as exchange rates seem to have a <strong>finite variance</strong>.</p>
<p>Techniques based on extreme values confirm that the index is generally:</p>
<ul class="simple">
<li><p>Strictly bigger than two</p></li>
<li><p>Lower or equal to five</p></li>
<li><p>Its value is typically around <strong>three</strong></p></li>
</ul>
</div>
<p><strong>Physical Meaning:</strong> With <span class="math notranslate nohighlight">\(\alpha \approx 3\)</span>, the variance exists but higher moments may not. This is heavier-tailed than normal but not as extreme as Cauchy.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 3: Up-Down Move Asymmetry</strong></p>
<p>One observes an <strong>up-down move asymmetry</strong>. Indeed, large drawdowns in stock prices are not compensated by equally large gains.</p>
</div>
<p><strong>Physical Meaning:</strong> Markets tend to fall faster than they rise. Losses often occur more suddenly and dramatically than gains.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 4: Convergence to Normality at Longer Scales</strong></p>
<p>As the time scale <span class="math notranslate nohighlight">\(\Delta t\)</span> increases, the distribution of the log returns looks more and more like a <strong>normal distribution</strong>.</p>
<p>This explains and partly justifies the widespread use of the normal distribution for the log returns.</p>
</div>
<p><strong>Physical Meaning:</strong> Over longer time horizons, the Central Limit Theorem kicks in and returns become more normally distributed due to aggregation.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 5: Intermittency</strong></p>
<p>Returns are highly variable at every time scale. This phenomenon is often called <strong>intermittency</strong>. The presence of heavy tails supports this statement.</p>
</div>
<p><strong>Physical Meaning:</strong> Volatility itself is not constant - periods of calm alternate with periods of turbulence.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 6: Volatility Clustering</strong></p>
<p>Researchers found evidence supporting the existence of a phenomenon that they named <strong>volatility clustering</strong>.</p>
<p>Indeed, they observed that measures of volatility have <strong>positive autocorrelations</strong> over several days.</p>
<p>This shows that <strong>high-volatility events are clustered in time</strong>.</p>
</div>
<p><strong>Physical Meaning:</strong> “Volatility begets volatility” - if the market is volatile today, it’s likely to be volatile tomorrow. This violates the assumption of i.i.d. returns.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 7: Slow Decay of Nonlinear Autocorrelations</strong></p>
<p>Unlike the linear autocorrelations of log returns, the autocorrelations of <strong>absolute log returns</strong>, which can be seen as a measure of nonlinear dependence, decay slowly as the time lag increases.</p>
<p>One can also look at the autocorrelation function of the <strong>squared returns</strong> that decays slowly over several days or even weeks.</p>
<p>This shows that the log returns are <strong>not independent</strong> and this contradicts the hypothesis that the increments of the log asset prices follow a random walk.</p>
</div>
<p><strong>Physical Meaning:</strong> While returns themselves show little autocorrelation, their absolute values or squares (measures of volatility) show strong persistence. This indicates nonlinear dependence.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Stylized Fact 8: Long-Range Dependence (Controversial)</strong></p>
<p>Some researchers claim that this proves the existence of <strong>long-range dependence</strong>.</p>
<p>A consequence of this statement is that the <strong>Markov property would not be satisfied</strong> in general.</p>
<p>However, there is <strong>no consensus</strong> on this matter and it is difficult to test whether a time series satisfies the Markov property.</p>
</div>
<p><strong>Physical Meaning:</strong> Some believe markets have very long memory in volatility. This is controversial and has implications for model selection.</p>
</section>
<section id="other-empirical-observations">
<h3>Other Empirical Observations<a class="headerlink" href="#other-empirical-observations" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Additional Stylized Facts:</strong></p>
<ul class="simple">
<li><p>The changes in volatility are generally <strong>negatively correlated</strong> with the returns (leverage effect)</p></li>
<li><p>The trading volume is <strong>positively correlated</strong> with volatility</p></li>
<li><p>An estimate of the volatility at a coarse time scale <strong>predicts fairly accurately</strong> the volatility at finer time scales</p></li>
</ul>
</div>
</section>
</section>
<section id="statistical-assumptions-and-their-limitations">
<h2>Statistical Assumptions and Their Limitations<a class="headerlink" href="#statistical-assumptions-and-their-limitations" title="Link to this heading">#</a></h2>
<p>Moreover, in relation with the discussion about the features of financial data, you have to be aware of the various hypotheses that one must make when applying common statistical methods.</p>
<p>One can identify <strong>three main assumptions</strong> that come up in statistical estimation of financial data:</p>
<ol class="arabic simple">
<li><p><strong>Stationarity</strong></p></li>
<li><p><strong>Ergodicity</strong></p></li>
<li><p><strong>Finite sample properties of estimators</strong></p></li>
</ol>
<section id="stationarity-in-practice">
<h3>Stationarity in Practice<a class="headerlink" href="#stationarity-in-practice" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Stationarity Assumptions:</strong></p>
<p>The log returns are generally assumed to be stationary because this makes the statistical properties invariant in time. In particular, it allows us to estimate moments of the returns.</p>
<p><strong>Problem:</strong> The data actually fail to be stationary in calendar time because of <strong>seasonality effects</strong>, such as:</p>
<ul class="simple">
<li><p>Intraday variability</p></li>
<li><p>Weekend effects</p></li>
<li><p>January effects</p></li>
<li><p>etc.</p></li>
</ul>
<p><strong>Solution:</strong> It is possible to correct this problem by changing the calendar into a <strong>business time</strong>, in which the data appear stationary.</p>
</div>
<p><strong>Physical Meaning:</strong> Real markets have patterns related to the calendar (Monday effect, etc.). By using “business time” instead of clock time, we can make the data closer to stationary.</p>
</section>
<section id="ergodicity-in-practice">
<h3>Ergodicity in Practice<a class="headerlink" href="#ergodicity-in-practice" title="Link to this heading">#</a></h3>
<p>Next, stationarity alone is not sufficient to prove the convergence of the sample mean estimator to the model expectation, and <strong>ergodicity is also required</strong> to ensure that the empirical average does indeed converge toward the mean of the stochastic process.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Non-Ergodic Models:</strong></p>
<p>For instance, there are some <strong>multifractal processes</strong> used in the modeling of high-frequency data which are <strong>not ergodic</strong>.</p>
</div>
</section>
<section id="finite-sample-considerations">
<h3>Finite Sample Considerations<a class="headerlink" href="#finite-sample-considerations" title="Link to this heading">#</a></h3>
<p>Finally, in Finance, sample sizes are typically <strong>small</strong>. Consequently, the error of the estimate of the mean is substantial and it is therefore important to pair the estimate with a <strong>confidence interval</strong>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Model Trade-offs:</strong></p>
<p>In this course, we discuss extensively the <strong>random walk model with normal residuals</strong> for the log returns of financial assets.</p>
<p><strong>Drawback:</strong> This model does not match the data’s observed features.</p>
<p><strong>Merits:</strong> Its tractability. In particular, its properties such as:</p>
<ul class="simple">
<li><p>Stationarity and independence of its increments</p></li>
<li><p>Ergodicity</p></li>
<li><p>Finiteness of all its moments</p></li>
</ul>
<p>make it possible to apply statistical methods to estimate the moments of its distribution.</p>
</div>
<p><strong>Physical Meaning:</strong> We use simplified models (like normal random walks) not because they’re perfect, but because they’re tractable and “good enough” for many applications. More realistic models are often too complex for practical use.</p>
</section>
</section>
<section id="practice-problems">
<h2>Practice Problems<a class="headerlink" href="#practice-problems" title="Link to this heading">#</a></h2>
<section id="problem-1">
<h3>Problem 1<a class="headerlink" href="#problem-1" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Problem 1: Indicator Process</p>
<p>Consider the process <span class="math notranslate nohighlight">\(\{X(t); t \geq 0\}\)</span> defined by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X(t) = \begin{cases}
1 &amp; \text{if } t \leq Y \\
0 &amp; \text{if } t &gt; Y
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is a uniformly distributed random variable on the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<ol class="arabic simple">
<li><p>Compute, for <span class="math notranslate nohighlight">\(t \in [0, 1]\)</span>, the first-order probability mass of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(f(t, x)\)</span>.</p></li>
<li><p>What is the first-order probability mass of <span class="math notranslate nohighlight">\(X\)</span> for <span class="math notranslate nohighlight">\(t &gt; 1\)</span>?</p></li>
<li><p>Give the expectation and the variance of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Compute the autocovariance function of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(C_X(t_1, t_2)\)</span>.</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(X\)</span> wide sense stationary? Is <span class="math notranslate nohighlight">\(X\)</span> strict sense stationary?</p></li>
<li><p>What is the distribution of an increment of <span class="math notranslate nohighlight">\(X\)</span>? Does <span class="math notranslate nohighlight">\(X\)</span> have stationary increments?</p></li>
</ol>
</div>
</section>
<section id="problem-2">
<h3>Problem 2<a class="headerlink" href="#problem-2" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Problem 2: Exponential Decay Process</p>
<p>Consider the stochastic process:</p>
<div class="math notranslate nohighlight">
\[
X(t) = e^{-Yt}, \quad \text{for } t \geq 0
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable with a uniform distribution on the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<ol class="arabic simple">
<li><p>Calculate the first-order density function of the process <span class="math notranslate nohighlight">\(\{X(t); t \geq 0\}\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(E[X(t)]\)</span> for <span class="math notranslate nohighlight">\(t \geq 0\)</span>.</p></li>
<li><p>Compute the auto-covariance function <span class="math notranslate nohighlight">\(C_X(t, t+s)\)</span> for <span class="math notranslate nohighlight">\(s, t \geq 0\)</span>.</p></li>
</ol>
</div>
</section>
<section id="problem-3">
<h3>Problem 3<a class="headerlink" href="#problem-3" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Problem 3: Centered Poisson Process</p>
<p>Consider the process <span class="math notranslate nohighlight">\(\{X(t); t \geq 0\}\)</span> defined by:</p>
<div class="math notranslate nohighlight">
\[
X(t) = N(t) - \lambda t
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is a Poisson process with rate <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<ol class="arabic simple">
<li><p>Compute, for <span class="math notranslate nohighlight">\(t_1, t_2, n_1, n_2 &gt; 0\)</span>, the second-order probability mass of <span class="math notranslate nohighlight">\(N\)</span>, <span class="math notranslate nohighlight">\(G(t_1, t_2; n_1, n_2)\)</span>.</p></li>
<li><p>Give the expectation and the variance of <span class="math notranslate nohighlight">\(X(t)\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(E[X(t_1)X(t_2)]\)</span> for <span class="math notranslate nohighlight">\(0 &lt; t_1, t_2\)</span>.</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(X\)</span> wide sense stationary? Is <span class="math notranslate nohighlight">\(X\)</span> strict sense stationary?</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(X\)</span> a martingale? Justify your answer.</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(\frac{X(t)}{t}\)</span> mean ergodic?</p></li>
</ol>
</div>
</section>
<section id="problem-4">
<h3>Problem 4<a class="headerlink" href="#problem-4" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Problem 4: Linear Growth with Random Rate</p>
<p>Consider the process <span class="math notranslate nohighlight">\(\{X(t); t \geq 0\}\)</span> defined by:</p>
<div class="math notranslate nohighlight">
\[
X(t) = te^Y
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is a random variable with a uniform distribution on <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<ol class="arabic simple">
<li><p>Compute, for <span class="math notranslate nohighlight">\(t_1, t_2 &gt; 0\)</span> and <span class="math notranslate nohighlight">\(x_1 &gt; 0, x_2 &gt; 0\)</span>, the second-order cumulative distribution function of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(G(t_1, t_2; x_1, x_2)\)</span>.</p></li>
<li><p>Compute the expectation and the variance of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(E[X(t_1)X(t_2)]\)</span> for <span class="math notranslate nohighlight">\(0 &lt; t_1 &lt; t_2\)</span>.</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(X\)</span> wide sense stationary? Is <span class="math notranslate nohighlight">\(X\)</span> strict sense stationary?</p></li>
<li><p>What is the distribution of an increment of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(X(t_2) - X(t_1)\)</span>, for <span class="math notranslate nohighlight">\(0 &lt; t_1 &lt; t_2\)</span> given?</p></li>
<li><p>Does <span class="math notranslate nohighlight">\(X\)</span> have stationary increments? Does <span class="math notranslate nohighlight">\(X\)</span> have independent increments?</p></li>
</ol>
</div>
</section>
<section id="problem-5">
<h3>Problem 5<a class="headerlink" href="#problem-5" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Problem 5: Linear Process with Random Coefficient</p>
<p>Consider the process <span class="math notranslate nohighlight">\(\{X(t); t \geq 0\}\)</span> defined by:</p>
<div class="math notranslate nohighlight">
\[
X(t) = 1 + tY
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is a continuous random variable with cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>, and such that <span class="math notranslate nohighlight">\(E[Y] &gt; 0\)</span>.</p>
<ol class="arabic simple">
<li><p>Compute, for <span class="math notranslate nohighlight">\(t_1, t_2 &gt; 0\)</span>, the second-order cumulative distribution function of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(G(t_1, t_2; x_1, x_2)\)</span>.</p></li>
<li><p>Give the expectation and the variance of <span class="math notranslate nohighlight">\(X\)</span> in terms of the expectation and variance of <span class="math notranslate nohighlight">\(Y\)</span> respectively.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(E[X(t_1)X(t_2)]\)</span> for <span class="math notranslate nohighlight">\(0 &lt; t_1, t_2\)</span> in terms of the expectation of <span class="math notranslate nohighlight">\(Y\)</span> and the expectation of <span class="math notranslate nohighlight">\(Y^2\)</span>.</p></li>
<li><p>Is <span class="math notranslate nohighlight">\(X\)</span> wide sense stationary? Is <span class="math notranslate nohighlight">\(X\)</span> strict sense stationary?</p></li>
<li><p>What is the distribution of an increment of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(X(t_2) - X(t_1)\)</span>, for <span class="math notranslate nohighlight">\(0 &lt; t_1 &lt; t_2\)</span> given?</p></li>
<li><p>Does <span class="math notranslate nohighlight">\(X\)</span> have stationary increments? Does <span class="math notranslate nohighlight">\(X\)</span> have independent increments?</p></li>
</ol>
</div>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this lecture, we covered:</p>
<ol class="arabic simple">
<li><p><strong>Basic Concepts:</strong></p>
<ul class="simple">
<li><p>Definition of stochastic processes</p></li>
<li><p>Sample paths and state spaces</p></li>
<li><p>Distribution functions of order <span class="math notranslate nohighlight">\(k\)</span></p></li>
</ul>
</li>
<li><p><strong>Key Properties:</strong></p>
<ul class="simple">
<li><p>Independent and stationary increments</p></li>
<li><p>Autocovariance and autocorrelation functions</p></li>
<li><p>Stationarity (strict sense and wide sense)</p></li>
<li><p>Ergodicity</p></li>
</ul>
</li>
<li><p><strong>Important Examples:</strong></p>
<ul class="simple">
<li><p>Symmetric random walk</p></li>
<li><p>Bernoulli process</p></li>
<li><p>AR(1) process</p></li>
<li><p>Gaussian processes</p></li>
</ul>
</li>
<li><p><strong>Empirical Stylized Facts:</strong></p>
<ul class="simple">
<li><p>Near-zero linear autocorrelations</p></li>
<li><p>Fat tails and power-law behavior</p></li>
<li><p>Volatility clustering</p></li>
<li><p>Asymmetry in up/down moves</p></li>
</ul>
</li>
<li><p><strong>Statistical Issues:</strong></p>
<ul class="simple">
<li><p>Stationarity assumptions and seasonality</p></li>
<li><p>Ergodicity requirements</p></li>
<li><p>Finite sample considerations</p></li>
</ul>
</li>
</ol>
<p>These concepts form the foundation for understanding financial time series and are essential for building and evaluating quantitative models in finance.</p>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Jean Jacod and Philip Protter, “Probability Essentials”, Universitext, Second Edition, 2004, Springer</p></li>
<li><p>Paul Wilmott, Sam Howison, and Jeff Dewynne, “Mathematics of Financial Derivatives: A Student Introduction”, Cambridge University Press, 1995</p></li>
<li><p>Rama Cont, “Empirical properties of asset returns: stylized facts and statistical issues”, Quantitative Finance, 1:223-236, 2001</p></li>
<li><p>Dan Stefanica, “A Linear Algebra Primer for Financial Engineering: Covariance Matrices, Eigenvectors, OLS, and more”, Financial Engineering Advanced Background Series, 2014, FE Press New York</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture05_binomial-asset-pricing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Binomial Asset Pricing Model</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture07_brownian-motion.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Continuous-Time Limit of the Random Walk</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-concepts-and-examples">Preliminary Concepts and Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-framework">The Framework</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-stochastic-process">Definition of Stochastic Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space">State Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-elementary-examples">Some Elementary Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-symmetric-random-walk">Example of the Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-step-increment">One-Step Increment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walk-definition">Random Walk Definition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-and-second-moments">First and Second Moments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-of-a-stochastic-process">Distribution of a Stochastic Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-function-of-order-k">Distribution Function of Order <span class="math notranslate nohighlight">\(k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-and-continuous-cases">Discrete and Continuous Cases</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-stationarity-of-increments">Independence and Stationarity of Increments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-the-bernoulli-process-revisited">An Example: The Bernoulli Process Revisited</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution-and-expectation">Distribution and Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-successes">Cumulative Successes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-stationarity">Independence and Stationarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#another-important-example-the-symmetric-random-walk">Another Important Example: The Symmetric Random Walk</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moments-of-increments">Moments of Increments</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-mean-and-variance">Overall Mean and Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autocovariance-and-autocorrelation-functions">Autocovariance and Autocorrelation Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-the-ar-1-process-for-a-1">Example of the AR(1) Process for <span class="math notranslate nohighlight">\(a = 1\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-autocovariance-function">Computing the Autocovariance Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strict-sense-and-wide-sense-stationarity">Strict Sense and Wide Sense Stationarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-between-sss-and-wss">Relationship Between SSS and WSS</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-stationarity">Examples of Stationarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-4-the-symmetric-random-walk">Example 4: The Symmetric Random Walk</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ar-1-process">The AR(1) Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explicit-form">Explicit Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-processes">The Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-normal-distribution">Multi-Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution">Joint Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#density-function">Density Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-covariance-matrix">Properties of Covariance Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes-and-stationarity">Gaussian Processes and Stationarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ergodicity">Ergodicity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-mean">Temporal Mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-ergodic-definition">Mean-Ergodic Definition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-ergodicity">Examples of Ergodicity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-features-of-real-financial-data">The Features of Real Financial Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-returns">Log Returns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stylized-facts">Stylized Facts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-index">Tail Index</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-empirical-observations">Other Empirical Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-assumptions-and-their-limitations">Statistical Assumptions and Their Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationarity-in-practice">Stationarity in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ergodicity-in-practice">Ergodicity in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-sample-considerations">Finite Sample Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-problems">Practice Problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1">Problem 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2">Problem 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3">Problem 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-4">Problem 4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-5">Problem 5</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Agnès Tourin
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>